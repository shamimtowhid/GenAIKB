{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/sharedrive/nafi/traffic/lib/python3.9/site-packages')\n",
    "\n",
    "import pandas as pd\n",
    "from chronos import ChronosPipeline\n",
    "import torch\n",
    "from table_evaluator import TableEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "from sdv.single_table import GaussianCopulaSynthesizer\n",
    "from sdv.metadata import Metadata\n",
    "import numpy as np\n",
    "from ctgan import CTGAN\n",
    "from ctgan import load_demo\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import xgboost as xgb\n",
    "from table_evaluator import TableEvaluator\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(511540, 25)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itu_test_df = pd.read_csv(\"/home/sharedrive/nafi/trafficp3/datasets/feature_selected_itu_test.csv\")\n",
    "itu_test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(511540, 23)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itu_test_df.drop(columns=[\"Init_Win_bytes_forward\", \"Flow_IAT_Min\"], inplace=True)\n",
    "itu_test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing columns: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 1395.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Negative Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Flow_Duration</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Total_Length_of_Bwd_Packets</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bwd_Packet_Length_Max</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bwd_Packet_Length_Mean</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Flow_Bytes/s</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Flow_Packets/s</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Flow_IAT_Mean</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Flow_IAT_Std</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Flow_IAT_Max</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Fwd_IAT_Total</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Fwd_IAT_Mean</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Fwd_IAT_Std</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Fwd_IAT_Max</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Fwd_IAT_Min</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Fwd_Packets/s</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bwd_Packets/s</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Max_Packet_Length</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Packet_Length_Mean</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Average_Packet_Size</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Avg_Bwd_Segment_Size</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Subflow_Bwd_Bytes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>min_seg_size_forward</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Feature  Negative Count\n",
       "0                 Flow_Duration               0\n",
       "1   Total_Length_of_Bwd_Packets               0\n",
       "2         Bwd_Packet_Length_Max               0\n",
       "3        Bwd_Packet_Length_Mean               0\n",
       "4                  Flow_Bytes/s               0\n",
       "5                Flow_Packets/s               0\n",
       "6                 Flow_IAT_Mean               0\n",
       "7                  Flow_IAT_Std               0\n",
       "8                  Flow_IAT_Max               0\n",
       "9                 Fwd_IAT_Total               0\n",
       "10                 Fwd_IAT_Mean               0\n",
       "11                  Fwd_IAT_Std               0\n",
       "12                  Fwd_IAT_Max               0\n",
       "13                  Fwd_IAT_Min               0\n",
       "14                Fwd_Packets/s               0\n",
       "15                Bwd_Packets/s               0\n",
       "16            Max_Packet_Length               0\n",
       "17           Packet_Length_Mean               0\n",
       "18          Average_Packet_Size               0\n",
       "19         Avg_Bwd_Segment_Size               0\n",
       "20            Subflow_Bwd_Bytes               0\n",
       "21         min_seg_size_forward               0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def replace_negatives_with_positive_mean(df):\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    for col in df_copy.columns:\n",
    "        if col == \"Label\":\n",
    "            continue\n",
    "        if pd.api.types.is_numeric_dtype(df_copy[col]):\n",
    "            # calculate the mean of positive values in the column\n",
    "            positive_values = df_copy[col][df_copy[col] >= 0]\n",
    "            if not positive_values.empty:\n",
    "                positive_mean = positive_values.mean()\n",
    "                # replace negatives with the mean of positive values\n",
    "                df_copy.loc[df_copy[col] < 0, col] = positive_mean\n",
    "            else:\n",
    "                print(f\"Column '{col}' has no positive values. Negatives remain unchanged.\")\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "itu_test_df = replace_negatives_with_positive_mean(itu_test_df)\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def analyze_negative_values(df, show_values=False, sample=10):\n",
    "    # Select only numeric columns to optimize processing\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    summary = []\n",
    "    \n",
    "    # Use tqdm for progress bar (only in Jupyter)\n",
    "    for col in tqdm(numeric_cols, desc=\"Processing columns\"):\n",
    "        negative_count = (df[col] < 0).sum()\n",
    "        result = {\"Feature\": col, \"Negative Count\": negative_count}\n",
    "        \n",
    "        if show_values and negative_count > 0:\n",
    "            negatives = df[col][df[col] < 0]\n",
    "            result[\"Sample Values\"] = negatives.head(sample).tolist()\n",
    "        \n",
    "        summary.append(result)\n",
    "    \n",
    "    return pd.DataFrame(summary)\n",
    "\n",
    "analyze_negative_values(itu_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_label_encoder(file_name):\n",
    "    save_path = \"/home/sharedrive/nafi/trafficp3/cicidis\"\n",
    "    with open(f'{save_path}/models/{file_name}_label_encoder.pkl', 'rb') as le_file:\n",
    "        return pickle.load(le_file)\n",
    "\n",
    "def load_scaler(file_name):\n",
    "    save_path = \"/home/sharedrive/nafi/trafficp3/cicidis\"\n",
    "    with open(f'{save_path}/models/{file_name}_scaler.pkl', 'rb') as scaler_file:\n",
    "        return pickle.load(scaler_file)\n",
    "\n",
    "def load_model(file_name, rounds, num_samples=1000, epochi=1500):\n",
    "    save_path = \"/home/sharedrive/nafi/trafficp3/cicidis\"\n",
    "    with open(f'{save_path}/models/{file_name}sample{num_samples}epoch{epochi}rounds{rounds}.pkl', 'rb') as model:\n",
    "        return pickle.load(model) \n",
    "\n",
    "\n",
    "def inference(test_df, load_file_name, rounds, num_sample=1000):\n",
    "    x_test = test_df.drop(columns=[\"Label\"])\n",
    "    y_test = test_df[\"Label\"]\n",
    "\n",
    "    scaler = load_scaler(load_file_name)\n",
    "    le = load_label_encoder(load_file_name)\n",
    "    RF_model = load_model(load_file_name, rounds)\n",
    "    \n",
    "    x_test = scaler.transform(x_test)\n",
    "    y_test = le.transform(y_test)\n",
    "    print(\"scaler and label encoder loaded & applied!!\")\n",
    "\n",
    "\n",
    "    pred_rf = RF_model.predict(x_test)\n",
    "\n",
    "    print(f\"Classification Report for Sample Size {num_sample}:\")\n",
    "    print(classification_report(y_test, pred_rf, digits=4, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaler and label encoder loaded & applied!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:    3.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Sample Size 1000:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "                   BENIGN     0.9600    0.9573    0.9587    410865\n",
      "                      Bot     0.0263    0.9774    0.0512       354\n",
      "                     DDoS     0.6391    0.9956    0.7784     23160\n",
      "            DoS_GoldenEye     0.9897    0.9833    0.9865      1861\n",
      "                 DoS_Hulk     0.8580    0.6623    0.7476     41626\n",
      "         DoS_Slowhttptest     0.9432    0.9527    0.9479       994\n",
      "            DoS_slowloris     0.9949    0.9284    0.9605      1048\n",
      "              FTP-Patator     0.9944    0.9861    0.9902      1436\n",
      "               Heartbleed     1.0000    1.0000    1.0000         2\n",
      "             Infiltration     1.0000    0.5000    0.6667         6\n",
      "                 PortScan     0.9992    0.4955    0.6625     28728\n",
      "              SSH-Patator     1.0000    0.4902    0.6579      1067\n",
      "  Web_Attack__Brute_Force     1.0000    0.1029    0.1867       272\n",
      "Web_Attack__Sql_Injection     1.0000    0.5000    0.6667         4\n",
      "          Web_Attack__XSS     0.2699    0.9573    0.4211       117\n",
      "\n",
      "                 accuracy                         0.9078    511540\n",
      "                macro avg     0.8450    0.7659    0.7122    511540\n",
      "             weighted avg     0.9389    0.9078    0.9151    511540\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inference(itu_test_df, 'synthetic_from_cicids_tvae', 11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaler and label encoder loaded & applied!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:    3.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Sample Size 1000:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "                   BENIGN     0.9890    0.9584    0.9734    410865\n",
      "                      Bot     0.0292    0.9802    0.0568       354\n",
      "                     DDoS     0.9931    0.9380    0.9648     23160\n",
      "            DoS_GoldenEye     0.9908    0.9850    0.9879      1861\n",
      "                 DoS_Hulk     0.8880    0.9147    0.9012     41626\n",
      "         DoS_Slowhttptest     0.9519    0.8763    0.9125       994\n",
      "            DoS_slowloris     0.9949    0.9265    0.9595      1048\n",
      "              FTP-Patator     0.9944    0.9861    0.9902      1436\n",
      "               Heartbleed     1.0000    1.0000    1.0000         2\n",
      "             Infiltration     1.0000    0.5000    0.6667         6\n",
      "                 PortScan     0.9939    0.9931    0.9935     28728\n",
      "              SSH-Patator     1.0000    0.4883    0.6562      1067\n",
      "  Web_Attack__Brute_Force     1.0000    0.1066    0.1927       272\n",
      "Web_Attack__Sql_Injection     0.0010    0.5000    0.0020         4\n",
      "          Web_Attack__XSS     0.3404    0.9573    0.5022       117\n",
      "\n",
      "                 accuracy                         0.9544    511540\n",
      "                macro avg     0.8111    0.8074    0.7173    511540\n",
      "             weighted avg     0.9804    0.9544    0.9664    511540\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inference(itu_test_df, 'synthetic_from_cicids_tvae', 22)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaler and label encoder loaded & applied!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:    3.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Sample Size 1000:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "                   BENIGN     0.9959    0.9906    0.9932    410865\n",
      "                      Bot     0.9427    0.6045    0.7367       354\n",
      "                     DDoS     0.9998    0.9994    0.9996     23160\n",
      "            DoS_GoldenEye     0.9872    0.9914    0.9893      1861\n",
      "                 DoS_Hulk     0.9183    0.9777    0.9470     41626\n",
      "         DoS_Slowhttptest     0.9812    0.9970    0.9890       994\n",
      "            DoS_slowloris     0.9990    1.0000    0.9995      1048\n",
      "              FTP-Patator     0.9951    0.9868    0.9909      1436\n",
      "               Heartbleed     1.0000    1.0000    1.0000         2\n",
      "             Infiltration     0.8333    0.8333    0.8333         6\n",
      "                 PortScan     0.9940    0.9999    0.9969     28728\n",
      "              SSH-Patator     0.9981    0.4958    0.6625      1067\n",
      "  Web_Attack__Brute_Force     0.9160    0.8419    0.8774       272\n",
      "Web_Attack__Sql_Injection     1.0000    0.5000    0.6667         4\n",
      "          Web_Attack__XSS     0.8440    0.7863    0.8142       117\n",
      "\n",
      "                 accuracy                         0.9891    511540\n",
      "                macro avg     0.9603    0.8670    0.8997    511540\n",
      "             weighted avg     0.9895    0.9891    0.9890    511540\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inference(itu_test_df, 'synthetic_from_cicids_tvae', 33)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
